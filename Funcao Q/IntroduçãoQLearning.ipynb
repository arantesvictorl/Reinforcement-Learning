{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iniciarQtable(ambiente):\n",
    "    # Cria uma tabela de zeros com tamanho de acordo com os espaços de observação e ação\n",
    "    tabelaQ = np.zeros((ambiente.observation_space.n, ambiente.action_space.n))\n",
    "    return tabelaQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politicaEpsilonGreedy(tabelaQ, estado, epsilon):\n",
    "    # Cria um número aleatório entre 0 e 1\n",
    "    num_aleatorio = random.uniform(0, 1)\n",
    "    # Se o número aleatório for maior que epsilon_greedy --> exploitation\n",
    "    if num_aleatorio > epsilon:\n",
    "        # Realiza a ação com o valor mais alto de um estado\n",
    "        action = np.argmax(tabelaQ[estado])\n",
    "    # else --> exploration\n",
    "    else:\n",
    "        # Toma uma ação aleatória\n",
    "        action = ambiente.action_space.sample()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politicaGreedy(tabelaQ, estado):\n",
    "    # Cria a política Greedy\n",
    "    # Exploitation: Toma uma ação com o maior valor do estado e action_value\n",
    "    action = np.argmax(tabelaQ[estado])\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar(episodiosTreino, epsilonMinimo, epsilonMaximo, taxaDecaimento, ambiente, maximoPassos, tabelaQ, taxaAprendizado, gamma):\n",
    "    for episode in range(episodiosTreino):\n",
    "        # Reduz epsilon (porque precisamos de cada vez menos exploração)\n",
    "        epsilon = epsilonMinimo + (epsilonMaximo - epsilonMinimo)*np.exp(-taxaDecaimento*episode)\n",
    "        # Reseta o ambiente\n",
    "        estado = ambiente.reset()\n",
    "        passosTempo = 0\n",
    "        concluido = False\n",
    "\n",
    "        # repete\n",
    "        for passosTempo in range(maximoPassos):\n",
    "            # Escolhe a ação At usando a política epsilon greedy\n",
    "            acao = politicaEpsilonGreedy(tabelaQ, estado, epsilon)\n",
    "\n",
    "            # Toma a ação At e observa Rt+1 e St+1\n",
    "            # Toma a ação (a) e observa o resultado de estado(s) e reward (r)\n",
    "            novoEstado, recompensa, concluido, info = ambiente.step(acao)\n",
    "\n",
    "            # Atualiza Q(s,a) = Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            tabelaQ[estado][acao] = tabelaQ[estado][acao] = tabelaQ[estado][acao] + taxaAprendizado * \\\n",
    "                (recompensa + gamma *\n",
    "                 np.max(tabelaQ[novoEstado]) - tabelaQ[estado][acao])\n",
    "\n",
    "            # Se pronto, termina o episódio\n",
    "            if concluido:\n",
    "                break\n",
    "            # Nosso estado é o novo estado\n",
    "            estado = novoEstado\n",
    "    return tabelaQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliarAgente(ambiente, maximoPassos, episodiosAvaliacao, tabelaQ):\n",
    "    recompensasTotais = []\n",
    "    for episodio in range(episodiosAvaliacao):\n",
    "        # if seed:\n",
    "        #     estado = ambiente.reset(seed=seed[episodio])\n",
    "        # else:\n",
    "        estado = ambiente.reset()\n",
    "        concluido = False\n",
    "        recompensasEpisodio = 0\n",
    "        for passosTempo in range(maximoPassos):\n",
    "            acao = np.argmax(tabelaQ[estado][:])\n",
    "            novoEstado, recompensa, concluido, info = ambiente.step(acao)\n",
    "            recompensasEpisodio += recompensa\n",
    "            if concluido:\n",
    "                break\n",
    "            estado = novoEstado\n",
    "        recompensasTotais.append(recompensasEpisodio)\n",
    "    mediaRecompensas = np.mean(recompensasTotais)\n",
    "    desvioPadrao = np.std(recompensasTotais)\n",
    "    return mediaRecompensas, desvioPadrao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verAgente(ambiente, tabelaQ):\n",
    "    estado = ambiente.reset()\n",
    "    for passosTempo in range(1000):\n",
    "        ambiente.render()\n",
    "        acao = np.argmax(tabelaQ[estado][:])\n",
    "        novoEstado, recompensa, concluido, info = ambiente.step(acao)\n",
    "        if concluido:\n",
    "            break\n",
    "        estado = novoEstado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de treino\n",
    "episodiosTreino = 100000      # Episódios de Treino\n",
    "taxaAprendizado = 0.7        # Taxa de Aprendizado\n",
    "\n",
    "# Parâmetros de avaliação\n",
    "episodiosAvaliacao = 100     # Episódios de Teste\n",
    "\n",
    "# Parâmetros de Ambiente\n",
    "ambienteid = \"FrozenLake-v1\"               # Nome do Ambiente\n",
    "maximoPassos = 99            # Máximo de passos por episódio\n",
    "gamma = 0.95                 # Taxa de desconto\n",
    "sementeAvaliacao = []\n",
    "# Parâmetros de Exploração\n",
    "epsilon = 1.0                # Taxa de Exploração\n",
    "epsilonMaximo = 1.0          # Probabilidade de Exploração no início\n",
    "epsilonMinimo = 0.05         # Probabilidade de Exploração Mínima\n",
    "taxaDecaimento = 0.005       # Taxa de Decaimento Exponencial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiente = gym.make(\"FrozenLake-v1\", desc=None,\n",
    "                    map_name=\"4x4\", is_slippery=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelaQ = iniciarQtable(ambiente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelaQ = treinar(episodiosTreino, epsilonMinimo, epsilonMaximo,\n",
    "                  taxaDecaimento, ambiente, maximoPassos, tabelaQ, taxaAprendizado, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediaAvaliacao, desvioPadrao = avaliarAgente(ambiente, maximoPassos, episodiosAvaliacao, tabelaQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00, 0.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"{mediaAvaliacao:.2f}, {desvioPadrao:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "verAgente(ambiente, tabelaQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "621d98dcf21193a583e48be5ef4bdada2c85182eac3c03523ae273d47954aacc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
